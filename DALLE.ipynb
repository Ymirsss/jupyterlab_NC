{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b54aa7-abda-4ac6-bda0-2996120ae391",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import torch\n",
    "import os\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "from dalle_pytorch import VQGanVAE, DALLE, DiscreteVAE\n",
    "from dalle_pytorch.tokenizer import tokenizer\n",
    "from einops import repeat\n",
    "from dalle_nc import DALLE, DiscreteVAE\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# model\n",
    "vae = DiscreteVAE(\n",
    "    image_size = 8,\n",
    "    num_layers = 3,\n",
    "    num_tokens = 8192,\n",
    "    codebook_dim = 1024,\n",
    "    hidden_dim = 64,\n",
    "    num_resnet_blocks = 1,\n",
    "    temperature = 0.9\n",
    ")\n",
    "\n",
    "dalle = DALLE(\n",
    "    dim = 1024,\n",
    "    vae = vae,                  # automatically infer (1) image sequence length and (2) number of image tokens\n",
    "    num_text_tokens = 100000,    # vocab size for text\n",
    "    text_seq_len = 256,         # text sequence length\n",
    "    depth = 12,                 # should aim to be 64\n",
    "    heads = 16,                 # attention heads\n",
    "    dim_head = 64,              # attention head dimension\n",
    "    attn_dropout = 0.1,         # attention dropout\n",
    "    ff_dropout = 0.1            # feedforward dropout\n",
    ")\n",
    "# [NeuralCoder] pytorch_inc_dynamic_quant for dalle [Beginning Line]\n",
    "if \"GraphModule\" not in str(type(dalle)):\n",
    "    from neural_compressor.conf.config import QuantConf\n",
    "    from neural_compressor.experimental import Quantization, common\n",
    "    quant_config = QuantConf()\n",
    "    quant_config.usr_cfg.quantization.approach = \"post_training_dynamic_quant\"\n",
    "    quant_config.usr_cfg.model.framework = \"pytorch\"\n",
    "    quantizer = Quantization(quant_config)\n",
    "    quantizer.model = common.Model(dalle)\n",
    "    dalle = quantizer()\n",
    "    dalle = dalle.model\n",
    "    dalle.eval()\n",
    "# [NeuralCoder] pytorch_inc_dynamic_quant for dalle [Ending Line]\n",
    "\n",
    "dalle.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026dacd7-bda9-42b2-aa2f-43d7ba13864b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# real data for DALLE image generation\n",
    "files = glob.glob(\"/home2/longxin/Neural_Coder_EXT/real_text.txt\")\n",
    "\n",
    "# create dataloader\n",
    "input_list = []\n",
    "with torch.no_grad():\n",
    "    count = 0\n",
    "    for file in files:\n",
    "        texts = open(file, 'r').read().split('\\n')\n",
    "        for text in texts:\n",
    "            print(text)\n",
    "\n",
    "            num_images = 1\n",
    "\n",
    "            top_k = 0.9\n",
    "\n",
    "            image_size = vae.image_size\n",
    "\n",
    "            texts = text.split('|')\n",
    "\n",
    "            for j, text in tqdm(enumerate(texts)):\n",
    "                text_tokens = tokenizer.tokenize([text], 256).to('cpu')\n",
    "\n",
    "                text_tokens = repeat(text_tokens, '() n -> b n', b=num_images)\n",
    "\n",
    "                for text_chunk in tqdm(text_tokens):\n",
    "                    d = {}\n",
    "                    d[\"text\"] = text_chunk\n",
    "                    d[\"filter_thres\"] = top_k\n",
    "                    input_list.append(d)\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.samples = input_list\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx], 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "dataset = MyDataset()\n",
    "dataloader = DataLoader(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4c6abc-2d70-49fa-a03f-ee7a6887c56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference\n",
    "with torch.no_grad():\n",
    "    for step, (inputs, labels) in enumerate(dataloader):\n",
    "        print(\"running inference ...\")\n",
    "        output = dalle(**inputs)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
