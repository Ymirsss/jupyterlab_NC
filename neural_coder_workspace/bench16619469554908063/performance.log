=> using pre-trained model 'alexnet'
/home2/longxin/ls/envs/jupyterlab-ext/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.
  warnings.warn(
/home2/longxin/ls/envs/jupyterlab-ext/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
using CPU, this will be slow
2022-08-31 19:56:24 [INFO] Pass query framework capability elapsed time: 605.94 ms
2022-08-31 19:56:24 [INFO] Get FP32 model baseline.
Test: [ 1/90]	Time  1.291 ( 1.291)	Loss 1.1467e+01 (1.1467e+01)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
Test: [11/90]	Time  0.227 ( 0.288)	Loss 1.0421e+01 (1.0861e+01)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.16)
Test: [21/90]	Time  0.228 ( 0.258)	Loss 1.1066e+01 (1.0875e+01)	Acc@1   0.00 (  0.00)	Acc@5   0.89 (  0.26)
Test: [31/90]	Time  0.199 ( 0.242)	Loss 1.1284e+01 (1.0869e+01)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.35)
Test: [41/90]	Time  0.197 ( 0.237)	Loss 1.1155e+01 (1.0885e+01)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.39)
Test: [51/90]	Time  0.217 ( 0.233)	Loss 1.0433e+01 (1.0834e+01)	Acc@1   0.00 (  0.02)	Acc@5   0.89 (  0.37)
Test: [61/90]	Time  0.224 ( 0.231)	Loss 1.0763e+01 (1.0813e+01)	Acc@1   0.00 (  0.03)	Acc@5   0.89 (  0.37)
Test: [71/90]	Time  0.206 ( 0.229)	Loss 1.0525e+01 (1.0806e+01)	Acc@1   0.00 (  0.03)	Acc@5   0.00 (  0.36)
Test: [81/90]	Time  0.232 ( 0.228)	Loss 1.0957e+01 (1.0818e+01)	Acc@1   0.00 (  0.02)	Acc@5   0.00 (  0.36)
 *   Acc@1 0.020 Acc@5 0.340
2022-08-31 19:56:44 [INFO] Save tuning history to /home2/longxin/Neural_Coder_EXT/nc_workspace/2022-08-31_19-56-12/./history.snapshot.
2022-08-31 19:56:44 [INFO] FP32 baseline is: [Accuracy: 0.0200, Duration (seconds): 19.8880]
2022-08-31 19:56:44 [WARNING] Please note that calibration sampling size 100 isn't divisible exactly by batch size 112. So the real sampling size is 112.
/home2/longxin/ls/envs/jupyterlab-ext/lib/python3.9/site-packages/torch/nn/quantized/_reference/modules/utils.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  torch.tensor(weight_qparams["scale"], dtype=torch.float, device=device))
/home2/longxin/ls/envs/jupyterlab-ext/lib/python3.9/site-packages/torch/nn/quantized/_reference/modules/utils.py:28: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  torch.tensor(weight_qparams["zero_point"], dtype=zero_point_dtype, device=device))
2022-08-31 19:56:48 [INFO] |********Mixed Precision Statistics*******|
2022-08-31 19:56:48 [INFO] +------------------------+--------+-------+
2022-08-31 19:56:48 [INFO] |        Op Type         | Total  |  INT8 |
2022-08-31 19:56:48 [INFO] +------------------------+--------+-------+
2022-08-31 19:56:48 [INFO] |  quantize_per_tensor   |   1    |   1   |
2022-08-31 19:56:48 [INFO] |       ConvReLU2d       |   5    |   5   |
2022-08-31 19:56:48 [INFO] |       MaxPool2d        |   3    |   3   |
2022-08-31 19:56:48 [INFO] |   AdaptiveAvgPool2d    |   1    |   1   |
2022-08-31 19:56:48 [INFO] |        flatten         |   1    |   1   |
2022-08-31 19:56:48 [INFO] |        Dropout         |   2    |   2   |
2022-08-31 19:56:48 [INFO] |       LinearReLU       |   2    |   2   |
2022-08-31 19:56:48 [INFO] |         Linear         |   1    |   1   |
2022-08-31 19:56:48 [INFO] |       dequantize       |   1    |   1   |
2022-08-31 19:56:48 [INFO] +------------------------+--------+-------+
2022-08-31 19:56:48 [INFO] Pass quantize model elapsed time: 3746.74 ms
Test: [ 1/90]	Time  1.250 ( 1.250)	Loss 1.1337e+01 (1.1337e+01)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
Test: [11/90]	Time  0.075 ( 0.179)	Loss 1.0395e+01 (1.0846e+01)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.08)
Test: [21/90]	Time  0.069 ( 0.175)	Loss 1.1095e+01 (1.0872e+01)	Acc@1   0.00 (  0.00)	Acc@5   0.89 (  0.26)
Test: [31/90]	Time  0.374 ( 0.168)	Loss 1.1311e+01 (1.0870e+01)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.35)
Test: [41/90]	Time  0.155 ( 0.166)	Loss 1.1116e+01 (1.0884e+01)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.35)
Test: [51/90]	Time  0.400 ( 0.171)	Loss 1.0456e+01 (1.0834e+01)	Acc@1   0.00 (  0.02)	Acc@5   0.89 (  0.33)
Test: [61/90]	Time  0.133 ( 0.168)	Loss 1.0771e+01 (1.0814e+01)	Acc@1   0.00 (  0.03)	Acc@5   0.89 (  0.32)
Test: [71/90]	Time  0.411 ( 0.167)	Loss 1.0504e+01 (1.0807e+01)	Acc@1   0.00 (  0.03)	Acc@5   0.00 (  0.33)
Test: [81/90]	Time  0.189 ( 0.166)	Loss 1.0933e+01 (1.0817e+01)	Acc@1   0.00 (  0.02)	Acc@5   0.00 (  0.31)
 *   Acc@1 0.020 Acc@5 0.280
2022-08-31 19:57:02 [INFO] Tune 1 result is: [Accuracy (int8|fp32): 0.0200|0.0200, Duration (seconds) (int8|fp32): 14.3324|19.8880], Best tune result is: [Accuracy: 0.0200, Duration (seconds): 14.3324]
2022-08-31 19:57:02 [INFO] |**********************Tune Result Statistics**********************|
2022-08-31 19:57:02 [INFO] +--------------------+----------+---------------+------------------+
2022-08-31 19:57:02 [INFO] |     Info Type      | Baseline | Tune 1 result | Best tune result |
2022-08-31 19:57:02 [INFO] +--------------------+----------+---------------+------------------+
2022-08-31 19:57:02 [INFO] |      Accuracy      | 0.0200   |    0.0200     |     0.0200       |
2022-08-31 19:57:02 [INFO] | Duration (seconds) | 19.8880  |    14.3324    |     14.3324      |
2022-08-31 19:57:02 [INFO] +--------------------+----------+---------------+------------------+
2022-08-31 19:57:02 [INFO] Save tuning history to /home2/longxin/Neural_Coder_EXT/nc_workspace/2022-08-31_19-56-12/./history.snapshot.
2022-08-31 19:57:02 [INFO] Specified timeout or max trials is reached! Found a quantized model which meet accuracy goal. Exit.
2022-08-31 19:57:03 [INFO] Save deploy yaml to /home2/longxin/Neural_Coder_EXT/nc_workspace/2022-08-31_19-56-12/deploy.yaml
Neural_Coder_Bench_IPS:  31.569
Neural_Coder_Bench_MSPI:  31.677
Neural_Coder_Bench_P50:  29.299
Neural_Coder_Bench_P90:  33.897
Neural_Coder_Bench_P99:  33.897
