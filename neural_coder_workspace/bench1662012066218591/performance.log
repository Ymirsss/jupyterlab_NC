import glob
0it [00:00, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 6159.04it/s]
1it [00:00, 338.61it/s]
import torch
0it [00:00, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 12122.27it/s]
1it [00:00, 851.98it/s]
import os
0it [00:00, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 12446.01it/s]
1it [00:00, 993.68it/s]
import sys
0it [00:00, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 13315.25it/s]
1it [00:00, 979.06it/s]
from tqdm import tqdm
0it [00:00, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 13443.28it/s]
1it [00:00, 957.82it/s]
from dalle_pytorch import VQGanVAE, DALLE, DiscreteVAE
0it [00:00, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 14513.16it/s]
1it [00:00, 634.25it/s]
from dalle_pytorch.tokenizer import tokenizer
0it [00:00, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 12985.46it/s]
1it [00:00, 858.43it/s]
from einops import repeat
0it [00:00, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 13706.88it/s]
1it [00:00, 901.03it/s]
from dalle_nc import DALLE, DiscreteVAE
0it [00:00, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 14413.42it/s]
1it [00:00, 1028.52it/s]
from torch.utils.data import DataLoader
0it [00:00, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 14979.66it/s]
1it [00:00, 825.00it/s]
from torch.utils.data import Dataset
0it [00:00, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 13934.56it/s]
1it [00:00, 953.90it/s]

0it [00:00, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 14563.56it/s]
1it [00:00, 1326.89it/s]
# model
0it [00:00, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 14122.24it/s]
1it [00:00, 1043.10it/s]
vae = DiscreteVAE(
0it [00:00, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 14873.42it/s]
1it [00:00, 1094.55it/s]
    image_size = 8,
0it [00:00, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 13400.33it/s]
1it [00:00, 934.77it/s]
    num_layers = 3,
0it [00:00, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 14463.12it/s]
1it [00:00, 978.83it/s]
    num_tokens = 8192,
0it [00:00, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 14463.12it/s]
1it [00:00, 980.66it/s]
    codebook_dim = 1024,
0it [00:00, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 14266.34it/s]
1it [00:00, 935.81it/s]
    hidden_dim = 64,
0it [00:00, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 12710.01it/s]
1it [00:00, 987.59it/s]
    num_resnet_blocks = 1,
0it [00:00, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 14266.34it/s]
1it [00:00, 940.43it/s]
    temperature = 0.9
0it [00:00, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 15033.35it/s]
1it [00:00, 941.06it/s]
)
0it [00:00, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 14364.05it/s]
1it [00:00, 1255.03it/s]

0it [00:00, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 15252.01it/s]
1it [00:00, 1435.42it/s]
dalle = DALLE(
0it [00:00, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 14315.03it/s]
1it [00:00, 1120.27it/s]
    dim = 1024,
0it [00:00, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 14716.86it/s]
1it [00:00, 1112.25it/s]
    vae = vae,                  # automatically infer (1) image sequence length and (2) number of image tokens
0it [00:00, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 11522.81it/s]
1it [00:00, 512.25it/s]
    num_text_tokens = 100000,    # vocab size for text
0it [00:00, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 13025.79it/s]
1it [00:00, 803.35it/s]
    text_seq_len = 256,         # text sequence length
0it [00:00, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 12710.01it/s]
1it [00:00, 925.49it/s]
    depth = 12,                 # should aim to be 64
0it [00:00, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 14315.03it/s]
1it [00:00, 884.87it/s]
    heads = 16,                 # attention heads
0it [00:00, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 13315.25it/s]
1it [00:00, 836.02it/s]
    dim_head = 64,              # attention head dimension
0it [00:00, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 13797.05it/s]
1it [00:00, 841.22it/s]
    attn_dropout = 0.1,         # attention dropout
0it [00:00, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 14315.03it/s]
1it [00:00, 908.64it/s]
    ff_dropout = 0.1            # feedforward dropout
0it [00:00, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 13530.01it/s]
1it [00:00, 885.62it/s]
)
0it [00:00, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 14820.86it/s]
1it [00:00, 1232.89it/s]

0it [00:00, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 14513.16it/s]
1it [00:00, 1383.80it/s]
dalle.eval()
0it [00:00, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 14614.30it/s]
1it [00:00, 1055.70it/s]

0it [00:00, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 14820.86it/s]
1it [00:00, 1440.35it/s]
# real data for DALLE image generation
0it [00:00, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 14315.03it/s]
1it [00:00, 890.32it/s]
files = glob.glob(os.path.dirname(os.path.realpath(sys.argv[0])) + "/real_text.txt")
0it [00:00, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 15087.42it/s]
1it [00:00, 670.12it/s]

0it [00:00, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 14979.66it/s]
1it [00:00, 1465.52it/s]
# create dataloader
0it [00:00, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 13662.23it/s]
1it [00:00, 1018.28it/s]
input_list = []
0it [00:00, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 14027.77it/s]
1it [00:00, 1014.34it/s]
with torch.no_grad():
0it [00:00, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 14665.40it/s]
1it [00:00, 993.20it/s]
    count = 0
0it [00:00, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 15033.35it/s]
1it [00:00, 1101.16it/s]
    for file in files:
0it [00:00, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 13148.29it/s]
1it [00:00, 1015.08it/s]
        texts = open(file, 'r').read().split('\n')
0it [00:00, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 14122.24it/s]
1it [00:00, 796.49it/s]
        for text in texts:
0it [00:00, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 15033.35it/s]
1it [00:00, 1166.70it/s]
            print(text)
0it [00:00, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 14665.40it/s]
1it [00:00, 1109.90it/s]

0it [00:00, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 13797.05it/s]
1it [00:00, 1453.83it/s]
            num_images = 1
0it [00:00, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 14315.03it/s]
1it [00:00, 1028.02it/s]

0it [00:00, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 14122.24it/s]
1it [00:00, 1362.67it/s]
            top_k = 0.9
0it [00:00, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 13934.56it/s]
1it [00:00, 1014.34it/s]

0it [00:00, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 15087.42it/s]
1it [00:00, 1418.43it/s]
            image_size = vae.image_size
0it [00:00, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 14563.56it/s]
1it [00:00, 1086.89it/s]

0it [00:00, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 15709.00it/s]
1it [00:00, 1475.31it/s]
            texts = text.split('|')
0it [00:00, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 13797.05it/s]

  0%|          | 0/1 [00:00<?, ?it/s][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 14169.95it/s]
2it [00:00, 1137.13it/s]

0it [00:00, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 14979.66it/s]
1it [00:00, 1397.64it/s]
            for j, text in tqdm(enumerate(texts)):
0it [00:00, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 13617.87it/s]
1it [00:00, 851.64it/s]
                text_tokens = tokenizer.tokenize([text], 256).to('cpu')
0it [00:00, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 13706.88it/s]
1it [00:00, 813.48it/s]

0it [00:00, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 14413.42it/s]
1it [00:00, 1424.70it/s]
                text_tokens = repeat(text_tokens, '() n -> b n', b=num_images)
0it [00:00, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 14122.24it/s]
1it [00:00, 919.00it/s]

0it [00:00, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 14926.35it/s]
1it [00:00, 1394.85it/s]
                for text_chunk in tqdm(text_tokens):
0it [00:00, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 12671.61it/s]
1it [00:00, 954.34it/s]
                    d = {}
0it [00:00, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 14217.98it/s]
1it [00:00, 1148.81it/s]
                    d["text"] = text_chunk
0it [00:00, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 14513.16it/s]
1it [00:00, 1100.58it/s]
                    d["filter_thres"] = top_k
0it [00:00, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 14820.86it/s]
1it [00:00, 968.44it/s]
                    input_list.append(d)
0it [00:00, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 13573.80it/s]
1it [00:00, 1001.03it/s]

0it [00:00, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 15650.39it/s]
1it [00:00, 1436.41it/s]
class MyDataset(Dataset):
0it [00:00, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 14820.86it/s]
1it [00:00, 907.66it/s]
    def __init__(self):
0it [00:00, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 15141.89it/s]
1it [00:00, 983.42it/s]
        self.samples = input_list
0it [00:00, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 14979.66it/s]
1it [00:00, 999.36it/s]

0it [00:00, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 14266.34it/s]
1it [00:00, 1393.92it/s]
    def __getitem__(self, idx):
0it [00:00, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 13934.56it/s]
1it [00:00, 935.81it/s]
        return self.samples[idx], 1
0it [00:00, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 14665.40it/s]
1it [00:00, 1007.04it/s]

0it [00:00, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 14513.16it/s]
1it [00:00, 1442.33it/s]
    def __len__(self):
0it [00:00, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 14266.34it/s]
1it [00:00, 1166.38it/s]
        return len(self.samples)
0it [00:00, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 12336.19it/s]
1it [00:00, 1052.79it/s]
dataset = MyDataset()
0it [00:00, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 14463.12it/s]
1it [00:00, 1111.66it/s]
dataloader = DataLoader(dataset)
0it [00:00, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 14027.77it/s]
1it [00:00, 1127.20it/s]

0it [00:00, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 14873.42it/s]
1it [00:00, 1452.32it/s]
# inference
0it [00:00, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 12336.19it/s]
1it [00:00, 871.09it/s]
with torch.no_grad():
0it [00:00, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 13706.88it/s]
1it [00:00, 1064.27it/s]
    for step, (inputs, labels) in enumerate(dataloader):
0it [00:00, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 14266.34it/s]
1it [00:00, 835.85it/s]
        print("running inference ...")
0it [00:00, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 12300.01it/s]
1it [00:00, 952.60it/s]
        output = dalle(**inputs)
0it [00:00, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 14513.16it/s]
1it [00:00, 1017.54it/s]

0it [00:00, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 15252.01it/s]
1it [00:00, 1450.81it/s]
2022-09-01 14:01:41 [INFO] Pass query framework capability elapsed time: 1208.59 ms
2022-09-01 14:01:41 [INFO] Get FP32 model baseline.
2022-09-01 14:01:41 [INFO] Save tuning history to /home2/longxin/Neural_Coder_EXT/nc_workspace/2022-09-01_14-01-29/./history.snapshot.
2022-09-01 14:01:41 [INFO] FP32 baseline is: [Accuracy: 1.0000, Duration (seconds): 0.0000]
/home2/longxin/ls/envs/jupyterlab-ext/lib/python3.9/site-packages/torch/ao/quantization/qconfig.py:92: UserWarning: QConfigDynamic is going to be deprecated in PyTorch 1.12, please use QConfig instead
  warnings.warn("QConfigDynamic is going to be deprecated in PyTorch 1.12, please use QConfig instead")
2022-09-01 14:01:41 [INFO] Fx trace of the entire model failed, We will conduct auto quantization
/home2/longxin/ls/envs/jupyterlab-ext/lib/python3.9/site-packages/torch/nn/quantized/_reference/modules/utils.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  torch.tensor(weight_qparams["scale"], dtype=torch.float, device=device))
/home2/longxin/ls/envs/jupyterlab-ext/lib/python3.9/site-packages/torch/nn/quantized/_reference/modules/utils.py:28: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  torch.tensor(weight_qparams["zero_point"], dtype=zero_point_dtype, device=device))
2022-09-01 14:02:24 [INFO] |*********Mixed Precision Statistics********|
2022-09-01 14:02:24 [INFO] +---------------------+-------+------+------+
2022-09-01 14:02:24 [INFO] |       Op Type       | Total | INT8 | FP32 |
2022-09-01 14:02:24 [INFO] +---------------------+-------+------+------+
2022-09-01 14:02:24 [INFO] |      Embedding      |   3   |  3   |  0   |
2022-09-01 14:02:24 [INFO] |      ConvReLU2d     |   7   |  0   |  7   |
2022-09-01 14:02:24 [INFO] |        Conv2d       |   5   |  0   |  5   |
2022-09-01 14:02:24 [INFO] |   ConvTranspose2d   |   3   |  0   |  3   |
2022-09-01 14:02:24 [INFO] |      LayerNorm      |   25  |  0   |  25  |
2022-09-01 14:02:24 [INFO] | quantize_per_tensor |   36  |  36  |  0   |
2022-09-01 14:02:24 [INFO] |        Linear       |   49  |  36  |  13  |
2022-09-01 14:02:24 [INFO] |      dequantize     |   36  |  36  |  0   |
2022-09-01 14:02:24 [INFO] |       Dropout       |   24  |  0   |  24  |
2022-09-01 14:02:24 [INFO] |        cache        |   12  |  12  |  0   |
2022-09-01 14:02:24 [INFO] |      cache_key      |   12  |  12  |  0   |
2022-09-01 14:02:24 [INFO] +---------------------+-------+------+------+
2022-09-01 14:02:24 [INFO] Pass quantize model elapsed time: 42672.2 ms
2022-09-01 14:02:24 [INFO] Tune 1 result is: [Accuracy (int8|fp32): 1.0000|1.0000, Duration (seconds) (int8|fp32): 0.0000|0.0000], Best tune result is: [Accuracy: 1.0000, Duration (seconds): 0.0000]
2022-09-01 14:02:24 [INFO] |**********************Tune Result Statistics**********************|
2022-09-01 14:02:24 [INFO] +--------------------+----------+---------------+------------------+
2022-09-01 14:02:24 [INFO] |     Info Type      | Baseline | Tune 1 result | Best tune result |
2022-09-01 14:02:24 [INFO] +--------------------+----------+---------------+------------------+
2022-09-01 14:02:24 [INFO] |      Accuracy      | 1.0000   |    1.0000     |     1.0000       |
2022-09-01 14:02:24 [INFO] | Duration (seconds) | 0.0000   |    0.0000     |     0.0000       |
2022-09-01 14:02:24 [INFO] +--------------------+----------+---------------+------------------+
2022-09-01 14:02:24 [INFO] Save tuning history to /home2/longxin/Neural_Coder_EXT/nc_workspace/2022-09-01_14-01-29/./history.snapshot.
2022-09-01 14:02:24 [INFO] Specified timeout or max trials is reached! Found a quantized model which meet accuracy goal. Exit.
2022-09-01 14:02:24 [INFO] Save deploy yaml to /home2/longxin/Neural_Coder_EXT/nc_workspace/2022-09-01_14-01-29/deploy.yaml
running inference ...
Neural_Coder_Bench_IPS:  3.635
Neural_Coder_Bench_MSPI:  275.07
Neural_Coder_Bench_P50:  272.384
Neural_Coder_Bench_P90:  293.515
Neural_Coder_Bench_P99:  293.515
