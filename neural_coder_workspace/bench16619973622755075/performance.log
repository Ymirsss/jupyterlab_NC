=> using pre-trained model 'alexnet'
/home2/longxin/ls/envs/jupyterlab-ext/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.
  warnings.warn(
/home2/longxin/ls/envs/jupyterlab-ext/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
using CPU, this will be slow
2022-09-01 09:56:30 [INFO] Pass query framework capability elapsed time: 258.97 ms
2022-09-01 09:56:30 [INFO] Get FP32 model baseline.
Test: [ 1/90]	Time  1.377 ( 1.377)	Loss 8.7625e+00 (8.7625e+00)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
Test: [11/90]	Time  0.179 ( 0.295)	Loss 8.7625e+00 (8.7625e+00)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
Test: [21/90]	Time  0.248 ( 0.254)	Loss 8.7625e+00 (8.7625e+00)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
Test: [31/90]	Time  0.240 ( 0.234)	Loss 8.7625e+00 (8.7625e+00)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
Test: [41/90]	Time  0.182 ( 0.226)	Loss 8.7625e+00 (8.7625e+00)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
Test: [51/90]	Time  0.178 ( 0.219)	Loss 8.7625e+00 (8.7625e+00)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
Test: [61/90]	Time  0.184 ( 0.214)	Loss 8.7625e+00 (8.7625e+00)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
Test: [71/90]	Time  0.194 ( 0.210)	Loss 8.7625e+00 (8.7625e+00)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
Test: [81/90]	Time  0.216 ( 0.208)	Loss 8.7625e+00 (8.7625e+00)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
 *   Acc@1 0.000 Acc@5 0.000
2022-09-01 09:56:48 [INFO] Save tuning history to /home2/longxin/Neural_Coder_EXT/nc_workspace/2022-09-01_09-56-18/./history.snapshot.
2022-09-01 09:56:48 [INFO] FP32 baseline is: [Accuracy: 0.0000, Duration (seconds): 18.0791]
2022-09-01 09:56:48 [WARNING] Please note that calibration sampling size 100 isn't divisible exactly by batch size 112. So the real sampling size is 112.
/home2/longxin/ls/envs/jupyterlab-ext/lib/python3.9/site-packages/torch/nn/quantized/_reference/modules/utils.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  torch.tensor(weight_qparams["scale"], dtype=torch.float, device=device))
/home2/longxin/ls/envs/jupyterlab-ext/lib/python3.9/site-packages/torch/nn/quantized/_reference/modules/utils.py:28: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  torch.tensor(weight_qparams["zero_point"], dtype=zero_point_dtype, device=device))
2022-09-01 09:56:51 [INFO] |********Mixed Precision Statistics*******|
2022-09-01 09:56:51 [INFO] +------------------------+--------+-------+
2022-09-01 09:56:51 [INFO] |        Op Type         | Total  |  INT8 |
2022-09-01 09:56:51 [INFO] +------------------------+--------+-------+
2022-09-01 09:56:51 [INFO] |  quantize_per_tensor   |   1    |   1   |
2022-09-01 09:56:51 [INFO] |       ConvReLU2d       |   5    |   5   |
2022-09-01 09:56:51 [INFO] |       MaxPool2d        |   3    |   3   |
2022-09-01 09:56:51 [INFO] |   AdaptiveAvgPool2d    |   1    |   1   |
2022-09-01 09:56:51 [INFO] |        flatten         |   1    |   1   |
2022-09-01 09:56:51 [INFO] |        Dropout         |   2    |   2   |
2022-09-01 09:56:51 [INFO] |       LinearReLU       |   2    |   2   |
2022-09-01 09:56:51 [INFO] |         Linear         |   1    |   1   |
2022-09-01 09:56:51 [INFO] |       dequantize       |   1    |   1   |
2022-09-01 09:56:51 [INFO] +------------------------+--------+-------+
2022-09-01 09:56:51 [INFO] Pass quantize model elapsed time: 2975.15 ms
Test: [ 1/90]	Time  1.040 ( 1.040)	Loss 8.5123e+00 (8.5123e+00)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
Test: [11/90]	Time  0.094 ( 0.188)	Loss 8.5123e+00 (8.5123e+00)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
Test: [21/90]	Time  0.063 ( 0.158)	Loss 8.5123e+00 (8.5123e+00)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
Test: [31/90]	Time  0.122 ( 0.161)	Loss 8.5123e+00 (8.5123e+00)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
Test: [41/90]	Time  0.099 ( 0.160)	Loss 8.5123e+00 (8.5123e+00)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
Test: [51/90]	Time  0.082 ( 0.168)	Loss 8.5123e+00 (8.5123e+00)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
Test: [61/90]	Time  0.079 ( 0.167)	Loss 8.5123e+00 (8.5123e+00)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
Test: [71/90]	Time  0.087 ( 0.170)	Loss 8.5123e+00 (8.5123e+00)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
Test: [81/90]	Time  0.054 ( 0.168)	Loss 8.5123e+00 (8.5123e+00)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
 *   Acc@1 0.000 Acc@5 0.000
2022-09-01 09:57:06 [INFO] Tune 1 result is: [Accuracy (int8|fp32): 0.0000|0.0000, Duration (seconds) (int8|fp32): 15.0699|18.0791], Best tune result is: [Accuracy: 0.0000, Duration (seconds): 15.0699]
2022-09-01 09:57:06 [INFO] |**********************Tune Result Statistics**********************|
2022-09-01 09:57:06 [INFO] +--------------------+----------+---------------+------------------+
2022-09-01 09:57:06 [INFO] |     Info Type      | Baseline | Tune 1 result | Best tune result |
2022-09-01 09:57:06 [INFO] +--------------------+----------+---------------+------------------+
2022-09-01 09:57:06 [INFO] |      Accuracy      | 0.0000   |    0.0000     |     0.0000       |
2022-09-01 09:57:06 [INFO] | Duration (seconds) | 18.0791  |    15.0699    |     15.0699      |
2022-09-01 09:57:06 [INFO] +--------------------+----------+---------------+------------------+
2022-09-01 09:57:06 [INFO] Save tuning history to /home2/longxin/Neural_Coder_EXT/nc_workspace/2022-09-01_09-56-18/./history.snapshot.
2022-09-01 09:57:06 [INFO] Specified timeout or max trials is reached! Found a quantized model which meet accuracy goal. Exit.
2022-09-01 09:57:06 [INFO] Save deploy yaml to /home2/longxin/Neural_Coder_EXT/nc_workspace/2022-09-01_09-56-18/deploy.yaml
Neural_Coder_Bench_IPS:  32.78
Neural_Coder_Bench_MSPI:  30.507
Neural_Coder_Bench_P50:  27.371
Neural_Coder_Bench_P90:  38.517
Neural_Coder_Bench_P99:  38.517
