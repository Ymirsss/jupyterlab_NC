=> using pre-trained model 'alexnet'
/home2/longxin/ls/envs/jupyterlab-ext/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.
  warnings.warn(
/home2/longxin/ls/envs/jupyterlab-ext/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
using CPU, this will be slow
2022-08-31 20:11:49 [INFO] Pass query framework capability elapsed time: 261.96 ms
2022-08-31 20:11:49 [INFO] Get FP32 model baseline.
Test: [ 1/90]	Time  1.398 ( 1.398)	Loss 1.1467e+01 (1.1467e+01)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
Test: [11/90]	Time  0.240 ( 0.332)	Loss 1.0421e+01 (1.0861e+01)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.16)
Test: [21/90]	Time  0.205 ( 0.284)	Loss 1.1066e+01 (1.0875e+01)	Acc@1   0.00 (  0.00)	Acc@5   0.89 (  0.26)
Test: [31/90]	Time  0.209 ( 0.261)	Loss 1.1284e+01 (1.0869e+01)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.35)
Test: [41/90]	Time  0.246 ( 0.256)	Loss 1.1155e+01 (1.0885e+01)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.39)
Test: [51/90]	Time  0.285 ( 0.252)	Loss 1.0433e+01 (1.0834e+01)	Acc@1   0.00 (  0.02)	Acc@5   0.89 (  0.37)
Test: [61/90]	Time  0.190 ( 0.249)	Loss 1.0763e+01 (1.0813e+01)	Acc@1   0.00 (  0.03)	Acc@5   0.89 (  0.37)
Test: [71/90]	Time  0.207 ( 0.247)	Loss 1.0525e+01 (1.0806e+01)	Acc@1   0.00 (  0.03)	Acc@5   0.00 (  0.36)
Test: [81/90]	Time  0.238 ( 0.244)	Loss 1.0957e+01 (1.0818e+01)	Acc@1   0.00 (  0.02)	Acc@5   0.00 (  0.36)
 *   Acc@1 0.020 Acc@5 0.340
2022-08-31 20:12:10 [INFO] Save tuning history to /home2/longxin/Neural_Coder_EXT/nc_workspace/2022-08-31_20-11-34/./history.snapshot.
2022-08-31 20:12:10 [INFO] FP32 baseline is: [Accuracy: 0.0200, Duration (seconds): 20.5490]
2022-08-31 20:12:10 [WARNING] Please note that calibration sampling size 100 isn't divisible exactly by batch size 112. So the real sampling size is 112.
/home2/longxin/ls/envs/jupyterlab-ext/lib/python3.9/site-packages/torch/nn/quantized/_reference/modules/utils.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  torch.tensor(weight_qparams["scale"], dtype=torch.float, device=device))
/home2/longxin/ls/envs/jupyterlab-ext/lib/python3.9/site-packages/torch/nn/quantized/_reference/modules/utils.py:28: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  torch.tensor(weight_qparams["zero_point"], dtype=zero_point_dtype, device=device))
2022-08-31 20:12:13 [INFO] |********Mixed Precision Statistics*******|
2022-08-31 20:12:13 [INFO] +------------------------+--------+-------+
2022-08-31 20:12:13 [INFO] |        Op Type         | Total  |  INT8 |
2022-08-31 20:12:13 [INFO] +------------------------+--------+-------+
2022-08-31 20:12:13 [INFO] |  quantize_per_tensor   |   1    |   1   |
2022-08-31 20:12:13 [INFO] |       ConvReLU2d       |   5    |   5   |
2022-08-31 20:12:13 [INFO] |       MaxPool2d        |   3    |   3   |
2022-08-31 20:12:13 [INFO] |   AdaptiveAvgPool2d    |   1    |   1   |
2022-08-31 20:12:13 [INFO] |        flatten         |   1    |   1   |
2022-08-31 20:12:13 [INFO] |        Dropout         |   2    |   2   |
2022-08-31 20:12:13 [INFO] |       LinearReLU       |   2    |   2   |
2022-08-31 20:12:13 [INFO] |         Linear         |   1    |   1   |
2022-08-31 20:12:13 [INFO] |       dequantize       |   1    |   1   |
2022-08-31 20:12:13 [INFO] +------------------------+--------+-------+
2022-08-31 20:12:13 [INFO] Pass quantize model elapsed time: 3021.63 ms
Test: [ 1/90]	Time  0.812 ( 0.812)	Loss 1.1337e+01 (1.1337e+01)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
Test: [11/90]	Time  0.054 ( 0.165)	Loss 1.0395e+01 (1.0846e+01)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.08)
Test: [21/90]	Time  0.141 ( 0.162)	Loss 1.1095e+01 (1.0872e+01)	Acc@1   0.00 (  0.00)	Acc@5   0.89 (  0.26)
Test: [31/90]	Time  0.277 ( 0.161)	Loss 1.1311e+01 (1.0870e+01)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.35)
Test: [41/90]	Time  0.063 ( 0.157)	Loss 1.1116e+01 (1.0884e+01)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.35)
Test: [51/90]	Time  0.360 ( 0.161)	Loss 1.0456e+01 (1.0834e+01)	Acc@1   0.00 (  0.02)	Acc@5   0.89 (  0.33)
Test: [61/90]	Time  0.083 ( 0.160)	Loss 1.0771e+01 (1.0814e+01)	Acc@1   0.00 (  0.03)	Acc@5   0.89 (  0.32)
Test: [71/90]	Time  0.212 ( 0.161)	Loss 1.0504e+01 (1.0807e+01)	Acc@1   0.00 (  0.03)	Acc@5   0.00 (  0.33)
Test: [81/90]	Time  0.063 ( 0.159)	Loss 1.0933e+01 (1.0817e+01)	Acc@1   0.00 (  0.02)	Acc@5   0.00 (  0.31)
 *   Acc@1 0.020 Acc@5 0.280
2022-08-31 20:12:27 [INFO] Tune 1 result is: [Accuracy (int8|fp32): 0.0200|0.0200, Duration (seconds) (int8|fp32): 14.0903|20.5490], Best tune result is: [Accuracy: 0.0200, Duration (seconds): 14.0903]
2022-08-31 20:12:27 [INFO] |**********************Tune Result Statistics**********************|
2022-08-31 20:12:27 [INFO] +--------------------+----------+---------------+------------------+
2022-08-31 20:12:27 [INFO] |     Info Type      | Baseline | Tune 1 result | Best tune result |
2022-08-31 20:12:27 [INFO] +--------------------+----------+---------------+------------------+
2022-08-31 20:12:27 [INFO] |      Accuracy      | 0.0200   |    0.0200     |     0.0200       |
2022-08-31 20:12:27 [INFO] | Duration (seconds) | 20.5490  |    14.0903    |     14.0903      |
2022-08-31 20:12:27 [INFO] +--------------------+----------+---------------+------------------+
2022-08-31 20:12:27 [INFO] Save tuning history to /home2/longxin/Neural_Coder_EXT/nc_workspace/2022-08-31_20-11-34/./history.snapshot.
2022-08-31 20:12:27 [INFO] Specified timeout or max trials is reached! Found a quantized model which meet accuracy goal. Exit.
2022-08-31 20:12:27 [INFO] Save deploy yaml to /home2/longxin/Neural_Coder_EXT/nc_workspace/2022-08-31_20-11-34/deploy.yaml
Neural_Coder_Bench_IPS:  17.572
Neural_Coder_Bench_MSPI:  56.91
Neural_Coder_Bench_P50:  27.082
Neural_Coder_Bench_P90:  171.424
Neural_Coder_Bench_P99:  171.424
