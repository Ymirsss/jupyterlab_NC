08/31/2022 18:39:23 - WARNING - __main__ - Process rank: -1, device: cpu, n_gpu: 0distributed training: False, 16-bits training: False
08/31/2022 18:39:23 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=0,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=-1,
log_level=-1,
log_level_replica=-1,
log_on_each_node=True,
logging_dir=result/runs/Aug31_18-39-22_mlp-spr-05,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_hf,
output_dir=result,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=result,
save_on_each_node=False,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_ipex=False,
use_legacy_prediction_loop=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
xpu_backend=None,
)
08/31/2022 18:39:25 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/2.4.0/datasets/glue/glue.py not found in cache or force_download set to True, downloading to /home2/longxin/.cache/huggingface/datasets/downloads/tmpmbvvlh5j
Downloading builder script:   0%|          | 0.00/7.78k [00:00<?, ?B/s]Downloading builder script: 28.8kB [00:00, 11.5MB/s]                   
08/31/2022 18:39:26 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/2.4.0/datasets/glue/glue.py in cache at /home2/longxin/.cache/huggingface/datasets/downloads/6d9bc094a0588d875caee4e51df39ab5d6b6316bf60695294827b02601d421a5.759f3e257a3fad0984d9f8ba9a26479d341795eb50fa64e4c1de40f1fc421313.py
08/31/2022 18:39:26 - INFO - datasets.utils.file_utils - creating metadata file for /home2/longxin/.cache/huggingface/datasets/downloads/6d9bc094a0588d875caee4e51df39ab5d6b6316bf60695294827b02601d421a5.759f3e257a3fad0984d9f8ba9a26479d341795eb50fa64e4c1de40f1fc421313.py
08/31/2022 18:39:27 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/2.4.0/datasets/glue/dataset_infos.json not found in cache or force_download set to True, downloading to /home2/longxin/.cache/huggingface/datasets/downloads/tmpdf_001mg
Downloading metadata:   0%|          | 0.00/4.47k [00:00<?, ?B/s]Downloading metadata: 28.7kB [00:00, 13.7MB/s]                   
08/31/2022 18:39:28 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/2.4.0/datasets/glue/dataset_infos.json in cache at /home2/longxin/.cache/huggingface/datasets/downloads/5d6a1e138f2b2212326041f144bbc329f7d40bd6202da0e3cb58c2eb1790801a.082d8848abcb8cddda90647ec069014ca338abd4f45e0a83c6df1ece0d45476a
08/31/2022 18:39:28 - INFO - datasets.utils.file_utils - creating metadata file for /home2/longxin/.cache/huggingface/datasets/downloads/5d6a1e138f2b2212326041f144bbc329f7d40bd6202da0e3cb58c2eb1790801a.082d8848abcb8cddda90647ec069014ca338abd4f45e0a83c6df1ece0d45476a
08/31/2022 18:39:29 - INFO - datasets.info - Loading Dataset Infos from /home2/longxin/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad
08/31/2022 18:39:29 - INFO - datasets.builder - Overwrite dataset info from restored data version.
08/31/2022 18:39:29 - INFO - datasets.info - Loading Dataset info from /home2/longxin/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad
08/31/2022 18:39:31 - WARNING - datasets.builder - Reusing dataset glue (/home2/longxin/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)
08/31/2022 18:39:31 - INFO - datasets.info - Loading Dataset info from /home2/longxin/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad
  0%|          | 0/3 [00:00<?, ?it/s] 33%|███▎      | 1/3 [00:00<00:00,  2.27it/s]100%|██████████| 3/3 [00:00<00:00,  6.25it/s]
[INFO|hub.py:600] 2022-08-31 18:39:32,651 >> https://huggingface.co/albert-base-v2/resolve/main/config.json not found in cache or force_download set to True, downloading to /home2/longxin/.cache/huggingface/transformers/tmpur2pyq_f
Downloading config.json:   0%|          | 0.00/684 [00:00<?, ?B/s]Downloading config.json: 100%|██████████| 684/684 [00:00<00:00, 511kB/s]
[INFO|hub.py:613] 2022-08-31 18:39:33,778 >> storing https://huggingface.co/albert-base-v2/resolve/main/config.json in cache at /home2/longxin/.cache/huggingface/transformers/e48be00f755a5f765e36a32885e8d6a573081df3321c9e19428d12abadf7dba2.b8f28145885741cf994c0e8a97b724f6c974460c297002145e48e511d2496e88
[INFO|hub.py:621] 2022-08-31 18:39:33,819 >> creating metadata file for /home2/longxin/.cache/huggingface/transformers/e48be00f755a5f765e36a32885e8d6a573081df3321c9e19428d12abadf7dba2.b8f28145885741cf994c0e8a97b724f6c974460c297002145e48e511d2496e88
[INFO|configuration_utils.py:681] 2022-08-31 18:39:33,912 >> loading configuration file https://huggingface.co/albert-base-v2/resolve/main/config.json from cache at /home2/longxin/.cache/huggingface/transformers/e48be00f755a5f765e36a32885e8d6a573081df3321c9e19428d12abadf7dba2.b8f28145885741cf994c0e8a97b724f6c974460c297002145e48e511d2496e88
[INFO|configuration_utils.py:730] 2022-08-31 18:39:34,408 >> Model config AlbertConfig {
  "_name_or_path": "albert-base-v2",
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "finetuning_task": "sst2",
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 12,
  "num_hidden_groups": 1,
  "num_hidden_layers": 12,
  "num_memory_blocks": 0,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.21.2",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|tokenization_auto.py:404] 2022-08-31 18:39:35,392 >> Could not locate the tokenizer configuration file, will try to use the model config instead.
[INFO|configuration_utils.py:681] 2022-08-31 18:39:36,378 >> loading configuration file https://huggingface.co/albert-base-v2/resolve/main/config.json from cache at /home2/longxin/.cache/huggingface/transformers/e48be00f755a5f765e36a32885e8d6a573081df3321c9e19428d12abadf7dba2.b8f28145885741cf994c0e8a97b724f6c974460c297002145e48e511d2496e88
[INFO|configuration_utils.py:730] 2022-08-31 18:39:36,380 >> Model config AlbertConfig {
  "_name_or_path": "albert-base-v2",
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 12,
  "num_hidden_groups": 1,
  "num_hidden_layers": 12,
  "num_memory_blocks": 0,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.21.2",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|hub.py:600] 2022-08-31 18:39:38,389 >> https://huggingface.co/albert-base-v2/resolve/main/spiece.model not found in cache or force_download set to True, downloading to /home2/longxin/.cache/huggingface/transformers/tmpnp2zvskx
Downloading spiece.model:   0%|          | 0.00/742k [00:00<?, ?B/s]Downloading spiece.model:   1%|          | 4.00k/742k [00:00<00:41, 18.1kB/s]Downloading spiece.model:   3%|▎         | 20.0k/742k [00:00<00:15, 48.6kB/s]Downloading spiece.model:   4%|▍         | 28.0k/742k [00:00<00:16, 43.1kB/s]Downloading spiece.model:   6%|▌         | 45.0k/742k [00:00<00:12, 56.3kB/s]Downloading spiece.model:   8%|▊         | 61.0k/742k [00:01<00:11, 62.0kB/s]Downloading spiece.model:  10%|█         | 77.0k/742k [00:01<00:10, 65.6kB/s]Downloading spiece.model:  15%|█▍        | 109k/742k [00:01<00:07, 91.1kB/s] Downloading spiece.model:  17%|█▋        | 125k/742k [00:01<00:07, 84.1kB/s]Downloading spiece.model:  19%|█▉        | 141k/742k [00:02<00:07, 80.1kB/s]Downloading spiece.model:  21%|██        | 157k/742k [00:02<00:07, 77.8kB/s]Downloading spiece.model:  25%|██▌       | 189k/742k [00:02<00:05, 97.8kB/s]Downloading spiece.model:  28%|██▊       | 205k/742k [00:02<00:06, 90.2kB/s]Downloading spiece.model:  32%|███▏      | 237k/742k [00:02<00:04, 106kB/s] Downloading spiece.model:  34%|███▍      | 253k/742k [00:03<00:05, 96.2kB/s]Downloading spiece.model:  38%|███▊      | 285k/742k [00:03<00:04, 111kB/s] Downloading spiece.model:  45%|████▍     | 333k/742k [00:03<00:02, 143kB/s]Downloading spiece.model:  49%|████▉     | 365k/742k [00:03<00:02, 141kB/s]Downloading spiece.model:  53%|█████▎    | 397k/742k [00:04<00:02, 141kB/s]Downloading spiece.model:  56%|█████▌    | 413k/742k [00:04<00:02, 122kB/s]Downloading spiece.model:  66%|██████▋   | 493k/742k [00:04<00:01, 194kB/s]Downloading spiece.model:  73%|███████▎  | 541k/742k [00:04<00:01, 198kB/s]Downloading spiece.model:  77%|███████▋  | 573k/742k [00:05<00:00, 182kB/s]Downloading spiece.model:  84%|████████▎ | 621k/742k [00:05<00:00, 192kB/s]Downloading spiece.model:  90%|█████████ | 669k/742k [00:05<00:00, 199kB/s]Downloading spiece.model:  97%|█████████▋| 717k/742k [00:05<00:00, 205kB/s]Downloading spiece.model: 100%|██████████| 742k/742k [00:05<00:00, 133kB/s]
[INFO|hub.py:613] 2022-08-31 18:39:45,228 >> storing https://huggingface.co/albert-base-v2/resolve/main/spiece.model in cache at /home2/longxin/.cache/huggingface/transformers/10be6ce6d3508f1fdce98a57a574283b47c055228c1235f8686f039287ff8174.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|hub.py:621] 2022-08-31 18:39:45,250 >> creating metadata file for /home2/longxin/.cache/huggingface/transformers/10be6ce6d3508f1fdce98a57a574283b47c055228c1235f8686f039287ff8174.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|hub.py:600] 2022-08-31 18:39:46,274 >> https://huggingface.co/albert-base-v2/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /home2/longxin/.cache/huggingface/transformers/tmpmx7ko5xy
Downloading tokenizer.json:   0%|          | 0.00/1.25M [00:00<?, ?B/s]Downloading tokenizer.json:   1%|          | 12.0k/1.25M [00:00<00:24, 53.5kB/s]Downloading tokenizer.json:   2%|▏         | 28.0k/1.25M [00:00<00:20, 63.0kB/s]Downloading tokenizer.json:   3%|▎         | 40.0k/1.25M [00:00<00:21, 58.5kB/s]Downloading tokenizer.json:   4%|▍         | 56.0k/1.25M [00:00<00:19, 63.2kB/s]Downloading tokenizer.json:   5%|▍         | 64.0k/1.25M [00:01<00:23, 53.0kB/s]Downloading tokenizer.json:   6%|▌         | 80.0k/1.25M [00:01<00:20, 59.1kB/s]Downloading tokenizer.json:   7%|▋         | 96.0k/1.25M [00:01<00:19, 62.9kB/s]Downloading tokenizer.json:  10%|▉         | 128k/1.25M [00:01<00:13, 87.7kB/s] Downloading tokenizer.json:  11%|█         | 144k/1.25M [00:02<00:14, 82.5kB/s]Downloading tokenizer.json:  12%|█▏        | 160k/1.25M [00:02<00:14, 78.9kB/s]Downloading tokenizer.json:  15%|█▍        | 192k/1.25M [00:02<00:11, 97.9kB/s]Downloading tokenizer.json:  16%|█▌        | 208k/1.25M [00:02<00:12, 89.6kB/s]Downloading tokenizer.json:  19%|█▊        | 240k/1.25M [00:03<00:10, 105kB/s] Downloading tokenizer.json:  21%|██        | 272k/1.25M [00:03<00:08, 115kB/s]Downloading tokenizer.json:  24%|██▎       | 304k/1.25M [00:03<00:08, 124kB/s]Downloading tokenizer.json:  26%|██▌       | 336k/1.25M [00:03<00:07, 129kB/s]Downloading tokenizer.json:  30%|██▉       | 384k/1.25M [00:03<00:05, 154kB/s]Downloading tokenizer.json:  34%|███▎      | 432k/1.25M [00:04<00:05, 172kB/s]Downloading tokenizer.json:  37%|███▋      | 480k/1.25M [00:04<00:04, 183kB/s]Downloading tokenizer.json:  42%|████▏     | 544k/1.25M [00:04<00:03, 214kB/s]Downloading tokenizer.json:  47%|████▋     | 608k/1.25M [00:04<00:02, 234kB/s]Downloading tokenizer.json:  54%|█████▎    | 688k/1.25M [00:05<00:02, 270kB/s]Downloading tokenizer.json:  61%|██████    | 784k/1.25M [00:05<00:01, 315kB/s]Downloading tokenizer.json:  69%|██████▊   | 880k/1.25M [00:05<00:01, 347kB/s]Downloading tokenizer.json:  77%|███████▋  | 992k/1.25M [00:05<00:00, 390kB/s]Downloading tokenizer.json:  80%|████████  | 1.01M/1.25M [00:05<00:00, 346kB/s]Downloading tokenizer.json:  95%|█████████▍| 1.19M/1.25M [00:06<00:00, 577kB/s]Downloading tokenizer.json: 100%|██████████| 1.25M/1.25M [00:06<00:00, 209kB/s]
[INFO|hub.py:613] 2022-08-31 18:39:53,665 >> storing https://huggingface.co/albert-base-v2/resolve/main/tokenizer.json in cache at /home2/longxin/.cache/huggingface/transformers/828a43aa4b9d07e2b7d3be7c6bc10a3ae6e16e8d9c3a0c557783639de9eaeb1b.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|hub.py:621] 2022-08-31 18:39:53,723 >> creating metadata file for /home2/longxin/.cache/huggingface/transformers/828a43aa4b9d07e2b7d3be7c6bc10a3ae6e16e8d9c3a0c557783639de9eaeb1b.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|tokenization_utils_base.py:1803] 2022-08-31 18:39:57,277 >> loading file https://huggingface.co/albert-base-v2/resolve/main/spiece.model from cache at /home2/longxin/.cache/huggingface/transformers/10be6ce6d3508f1fdce98a57a574283b47c055228c1235f8686f039287ff8174.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d
[INFO|tokenization_utils_base.py:1803] 2022-08-31 18:39:57,277 >> loading file https://huggingface.co/albert-base-v2/resolve/main/tokenizer.json from cache at /home2/longxin/.cache/huggingface/transformers/828a43aa4b9d07e2b7d3be7c6bc10a3ae6e16e8d9c3a0c557783639de9eaeb1b.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74
[INFO|tokenization_utils_base.py:1803] 2022-08-31 18:39:57,277 >> loading file https://huggingface.co/albert-base-v2/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1803] 2022-08-31 18:39:57,278 >> loading file https://huggingface.co/albert-base-v2/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1803] 2022-08-31 18:39:57,278 >> loading file https://huggingface.co/albert-base-v2/resolve/main/tokenizer_config.json from cache at None
[INFO|configuration_utils.py:681] 2022-08-31 18:39:58,238 >> loading configuration file https://huggingface.co/albert-base-v2/resolve/main/config.json from cache at /home2/longxin/.cache/huggingface/transformers/e48be00f755a5f765e36a32885e8d6a573081df3321c9e19428d12abadf7dba2.b8f28145885741cf994c0e8a97b724f6c974460c297002145e48e511d2496e88
[INFO|configuration_utils.py:730] 2022-08-31 18:39:58,240 >> Model config AlbertConfig {
  "_name_or_path": "albert-base-v2",
  "architectures": [
    "AlbertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0,
  "bos_token_id": 2,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 3,
  "gap_size": 0,
  "hidden_act": "gelu_new",
  "hidden_dropout_prob": 0,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 12,
  "num_hidden_groups": 1,
  "num_hidden_layers": 12,
  "num_memory_blocks": 0,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.21.2",
  "type_vocab_size": 2,
  "vocab_size": 30000
}

[INFO|hub.py:600] 2022-08-31 18:40:00,148 >> https://huggingface.co/albert-base-v2/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /home2/longxin/.cache/huggingface/transformers/tmpb7or43qp
Downloading pytorch_model.bin:   0%|          | 0.00/45.2M [00:00<?, ?B/s]Downloading pytorch_model.bin:   0%|          | 35.0k/45.2M [00:00<03:33, 221kB/s]Downloading pytorch_model.bin:   0%|          | 110k/45.2M [00:00<02:02, 384kB/s] Downloading pytorch_model.bin:   0%|          | 212k/45.2M [00:00<01:16, 616kB/s]Downloading pytorch_model.bin:   1%|          | 335k/45.2M [00:00<00:57, 819kB/s]Downloading pytorch_model.bin:   1%|          | 537k/45.2M [00:00<00:39, 1.20MB/s]Downloading pytorch_model.bin:   2%|▏         | 900k/45.2M [00:00<00:23, 1.97MB/s]Downloading pytorch_model.bin:   2%|▏         | 1.08M/45.2M [00:00<00:30, 1.53MB/s]Downloading pytorch_model.bin:   4%|▍         | 2.00M/45.2M [00:01<00:15, 2.98MB/s]Downloading pytorch_model.bin:   7%|▋         | 3.00M/45.2M [00:01<00:11, 3.85MB/s]Downloading pytorch_model.bin:   9%|▉         | 4.00M/45.2M [00:01<00:09, 4.41MB/s]Downloading pytorch_model.bin:  11%|█         | 5.00M/45.2M [00:01<00:08, 4.68MB/s]Downloading pytorch_model.bin:  13%|█▎        | 6.00M/45.2M [00:01<00:07, 5.28MB/s]Downloading pytorch_model.bin:  15%|█▌        | 7.00M/45.2M [00:02<00:07, 5.64MB/s]Downloading pytorch_model.bin:  18%|█▊        | 8.00M/45.2M [00:02<00:06, 5.96MB/s]Downloading pytorch_model.bin:  20%|█▉        | 9.00M/45.2M [00:02<00:06, 6.21MB/s]Downloading pytorch_model.bin:  22%|██▏       | 10.0M/45.2M [00:02<00:05, 6.25MB/s]Downloading pytorch_model.bin:  24%|██▍       | 11.0M/45.2M [00:02<00:05, 6.24MB/s]Downloading pytorch_model.bin:  27%|██▋       | 12.0M/45.2M [00:02<00:05, 6.22MB/s]Downloading pytorch_model.bin:  29%|██▉       | 13.0M/45.2M [00:02<00:05, 6.54MB/s]Downloading pytorch_model.bin:  31%|███       | 14.0M/45.2M [00:03<00:05, 6.41MB/s]Downloading pytorch_model.bin:  33%|███▎      | 15.0M/45.2M [00:03<00:04, 6.72MB/s]Downloading pytorch_model.bin:  35%|███▌      | 16.0M/45.2M [00:03<00:04, 6.80MB/s]Downloading pytorch_model.bin:  38%|███▊      | 17.0M/45.2M [00:03<00:04, 6.98MB/s]Downloading pytorch_model.bin:  40%|███▉      | 18.0M/45.2M [00:03<00:04, 6.81MB/s]Downloading pytorch_model.bin:  42%|████▏     | 19.0M/45.2M [00:03<00:04, 6.74MB/s]Downloading pytorch_model.bin:  44%|████▍     | 20.0M/45.2M [00:04<00:03, 7.13MB/s]Downloading pytorch_model.bin:  46%|████▋     | 21.0M/45.2M [00:04<00:03, 7.19MB/s]Downloading pytorch_model.bin:  49%|████▊     | 22.0M/45.2M [00:04<00:03, 6.35MB/s]Downloading pytorch_model.bin:  51%|█████     | 23.0M/45.2M [00:04<00:03, 6.18MB/s]Downloading pytorch_model.bin:  53%|█████▎    | 24.0M/45.2M [00:04<00:03, 6.00MB/s]Downloading pytorch_model.bin:  55%|█████▌    | 25.0M/45.2M [00:04<00:03, 6.16MB/s]Downloading pytorch_model.bin:  58%|█████▊    | 26.0M/45.2M [00:05<00:03, 6.46MB/s]Downloading pytorch_model.bin:  60%|█████▉    | 27.0M/45.2M [00:05<00:02, 6.45MB/s]Downloading pytorch_model.bin:  62%|██████▏   | 28.0M/45.2M [00:05<00:02, 6.07MB/s]Downloading pytorch_model.bin:  64%|██████▍   | 29.0M/45.2M [00:05<00:02, 6.60MB/s]Downloading pytorch_model.bin:  66%|██████▋   | 30.0M/45.2M [00:05<00:02, 6.83MB/s]Downloading pytorch_model.bin:  69%|██████▊   | 31.0M/45.2M [00:05<00:02, 7.02MB/s]Downloading pytorch_model.bin:  71%|███████   | 32.0M/45.2M [00:05<00:01, 7.03MB/s]Downloading pytorch_model.bin:  73%|███████▎  | 33.0M/45.2M [00:06<00:01, 7.35MB/s]Downloading pytorch_model.bin:  75%|███████▌  | 34.0M/45.2M [00:06<00:01, 6.64MB/s]Downloading pytorch_model.bin:  77%|███████▋  | 35.0M/45.2M [00:06<00:01, 6.57MB/s]Downloading pytorch_model.bin:  80%|███████▉  | 36.0M/45.2M [00:06<00:01, 6.80MB/s]Downloading pytorch_model.bin:  82%|████████▏ | 37.0M/45.2M [00:06<00:01, 6.97MB/s]Downloading pytorch_model.bin:  84%|████████▍ | 38.0M/45.2M [00:06<00:01, 6.68MB/s]Downloading pytorch_model.bin:  86%|████████▋ | 39.0M/45.2M [00:07<00:00, 6.53MB/s]Downloading pytorch_model.bin:  89%|████████▊ | 40.0M/45.2M [00:07<00:00, 6.66MB/s]Downloading pytorch_model.bin:  91%|█████████ | 41.0M/45.2M [00:07<00:00, 6.60MB/s]Downloading pytorch_model.bin:  93%|█████████▎| 42.0M/45.2M [00:07<00:00, 6.76MB/s]Downloading pytorch_model.bin:  95%|█████████▌| 43.0M/45.2M [00:07<00:00, 6.60MB/s]Downloading pytorch_model.bin:  97%|█████████▋| 44.0M/45.2M [00:07<00:00, 6.49MB/s]Downloading pytorch_model.bin: 100%|█████████▉| 45.0M/45.2M [00:07<00:00, 6.95MB/s]Downloading pytorch_model.bin: 100%|██████████| 45.2M/45.2M [00:07<00:00, 5.93MB/s]
[INFO|hub.py:613] 2022-08-31 18:40:08,819 >> storing https://huggingface.co/albert-base-v2/resolve/main/pytorch_model.bin in cache at /home2/longxin/.cache/huggingface/transformers/bf1986d976e9a8320cbd3a0597e610bf299d639ce31b7ca581cbf54be3aaa6d3.d6d54047dfe6ae844e3bf6e7a7d0aff71cb598d3df019361e076ba7639b1da9b
[INFO|hub.py:621] 2022-08-31 18:40:08,825 >> creating metadata file for /home2/longxin/.cache/huggingface/transformers/bf1986d976e9a8320cbd3a0597e610bf299d639ce31b7ca581cbf54be3aaa6d3.d6d54047dfe6ae844e3bf6e7a7d0aff71cb598d3df019361e076ba7639b1da9b
[INFO|modeling_utils.py:2041] 2022-08-31 18:40:08,885 >> loading weights file https://huggingface.co/albert-base-v2/resolve/main/pytorch_model.bin from cache at /home2/longxin/.cache/huggingface/transformers/bf1986d976e9a8320cbd3a0597e610bf299d639ce31b7ca581cbf54be3aaa6d3.d6d54047dfe6ae844e3bf6e7a7d0aff71cb598d3df019361e076ba7639b1da9b
[WARNING|modeling_utils.py:2425] 2022-08-31 18:40:13,324 >> Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertForSequenceClassification: ['predictions.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.dense.weight']
- This IS expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:2437] 2022-08-31 18:40:13,324 >> Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Running tokenizer on dataset:   0%|          | 0/68 [00:00<?, ?ba/s]08/31/2022 18:40:13 - INFO - datasets.arrow_dataset - Caching processed dataset at /home2/longxin/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-bc2c4fa1eb57ca42.arrow
Running tokenizer on dataset:   1%|▏         | 1/68 [00:01<01:43,  1.55s/ba]Running tokenizer on dataset:   3%|▎         | 2/68 [00:02<01:09,  1.06s/ba]Running tokenizer on dataset:   4%|▍         | 3/68 [00:03<00:59,  1.08ba/s]Running tokenizer on dataset:   6%|▌         | 4/68 [00:03<00:50,  1.26ba/s]Running tokenizer on dataset:   7%|▋         | 5/68 [00:04<00:45,  1.40ba/s]Running tokenizer on dataset:   9%|▉         | 6/68 [00:04<00:42,  1.47ba/s]Running tokenizer on dataset:  10%|█         | 7/68 [00:05<00:37,  1.61ba/s]Running tokenizer on dataset:  12%|█▏        | 8/68 [00:05<00:36,  1.66ba/s]Running tokenizer on dataset:  13%|█▎        | 9/68 [00:06<00:35,  1.68ba/s]Running tokenizer on dataset:  15%|█▍        | 10/68 [00:06<00:33,  1.74ba/s]Running tokenizer on dataset:  16%|█▌        | 11/68 [00:07<00:35,  1.61ba/s]Running tokenizer on dataset:  18%|█▊        | 12/68 [00:08<00:34,  1.61ba/s]Running tokenizer on dataset:  19%|█▉        | 13/68 [00:08<00:33,  1.64ba/s]Running tokenizer on dataset:  21%|██        | 14/68 [00:09<00:31,  1.69ba/s]Running tokenizer on dataset:  22%|██▏       | 15/68 [00:10<00:32,  1.64ba/s]Running tokenizer on dataset:  24%|██▎       | 16/68 [00:10<00:30,  1.70ba/s]Running tokenizer on dataset:  25%|██▌       | 17/68 [00:11<00:30,  1.67ba/s]Running tokenizer on dataset:  26%|██▋       | 18/68 [00:11<00:30,  1.65ba/s]Running tokenizer on dataset:  28%|██▊       | 19/68 [00:12<00:28,  1.71ba/s]Running tokenizer on dataset:  29%|██▉       | 20/68 [00:12<00:27,  1.77ba/s]Running tokenizer on dataset:  31%|███       | 21/68 [00:13<00:26,  1.79ba/s]Running tokenizer on dataset:  32%|███▏      | 22/68 [00:14<00:25,  1.80ba/s]Running tokenizer on dataset:  34%|███▍      | 23/68 [00:14<00:26,  1.72ba/s]Running tokenizer on dataset:  35%|███▌      | 24/68 [00:15<00:26,  1.69ba/s]Running tokenizer on dataset:  37%|███▋      | 25/68 [00:15<00:24,  1.74ba/s]Running tokenizer on dataset:  38%|███▊      | 26/68 [00:16<00:23,  1.80ba/s]Running tokenizer on dataset:  40%|███▉      | 27/68 [00:17<00:24,  1.67ba/s]Running tokenizer on dataset:  41%|████      | 28/68 [00:17<00:22,  1.75ba/s]Running tokenizer on dataset:  43%|████▎     | 29/68 [00:18<00:22,  1.70ba/s]Running tokenizer on dataset:  44%|████▍     | 30/68 [00:18<00:22,  1.71ba/s]Running tokenizer on dataset:  46%|████▌     | 31/68 [00:19<00:21,  1.74ba/s]Running tokenizer on dataset:  47%|████▋     | 32/68 [00:19<00:20,  1.79ba/s]Running tokenizer on dataset:  49%|████▊     | 33/68 [00:20<00:19,  1.82ba/s]Running tokenizer on dataset:  50%|█████     | 34/68 [00:20<00:19,  1.76ba/s]Running tokenizer on dataset:  51%|█████▏    | 35/68 [00:21<00:19,  1.70ba/s]Running tokenizer on dataset:  53%|█████▎    | 36/68 [00:22<00:18,  1.72ba/s]Running tokenizer on dataset:  54%|█████▍    | 37/68 [00:22<00:19,  1.61ba/s]Running tokenizer on dataset:  56%|█████▌    | 38/68 [00:23<00:17,  1.71ba/s]Running tokenizer on dataset:  57%|█████▋    | 39/68 [00:23<00:17,  1.68ba/s]Running tokenizer on dataset:  59%|█████▉    | 40/68 [00:24<00:16,  1.66ba/s]Running tokenizer on dataset:  60%|██████    | 41/68 [00:25<00:15,  1.70ba/s]Running tokenizer on dataset:  62%|██████▏   | 42/68 [00:25<00:15,  1.72ba/s]Running tokenizer on dataset:  63%|██████▎   | 43/68 [00:26<00:14,  1.75ba/s]Running tokenizer on dataset:  65%|██████▍   | 44/68 [00:26<00:13,  1.78ba/s]Running tokenizer on dataset:  66%|██████▌   | 45/68 [00:27<00:13,  1.76ba/s]Running tokenizer on dataset:  68%|██████▊   | 46/68 [00:27<00:12,  1.79ba/s]Running tokenizer on dataset:  69%|██████▉   | 47/68 [00:28<00:11,  1.75ba/s]Running tokenizer on dataset:  71%|███████   | 48/68 [00:29<00:11,  1.74ba/s]Running tokenizer on dataset:  72%|███████▏  | 49/68 [00:29<00:10,  1.80ba/s]Running tokenizer on dataset:  74%|███████▎  | 50/68 [00:30<00:10,  1.74ba/s]Running tokenizer on dataset:  75%|███████▌  | 51/68 [00:30<00:09,  1.74ba/s]Running tokenizer on dataset:  76%|███████▋  | 52/68 [00:31<00:10,  1.50ba/s]Running tokenizer on dataset:  78%|███████▊  | 53/68 [00:32<00:09,  1.61ba/s]Running tokenizer on dataset:  79%|███████▉  | 54/68 [00:32<00:08,  1.58ba/s]Running tokenizer on dataset:  81%|████████  | 55/68 [00:33<00:07,  1.66ba/s]Running tokenizer on dataset:  82%|████████▏ | 56/68 [00:33<00:06,  1.72ba/s]Running tokenizer on dataset:  84%|████████▍ | 57/68 [00:34<00:06,  1.76ba/s]Running tokenizer on dataset:  85%|████████▌ | 58/68 [00:35<00:05,  1.73ba/s]Running tokenizer on dataset:  87%|████████▋ | 59/68 [00:35<00:05,  1.78ba/s]Running tokenizer on dataset:  88%|████████▊ | 60/68 [00:36<00:04,  1.81ba/s]Running tokenizer on dataset:  90%|████████▉ | 61/68 [00:36<00:03,  1.76ba/s]Running tokenizer on dataset:  91%|█████████ | 62/68 [00:37<00:03,  1.74ba/s]Running tokenizer on dataset:  93%|█████████▎| 63/68 [00:38<00:03,  1.56ba/s]Running tokenizer on dataset:  94%|█████████▍| 64/68 [00:38<00:02,  1.60ba/s]Running tokenizer on dataset:  96%|█████████▌| 65/68 [00:39<00:01,  1.66ba/s]Running tokenizer on dataset:  97%|█████████▋| 66/68 [00:39<00:01,  1.72ba/s]Running tokenizer on dataset:  99%|█████████▊| 67/68 [00:40<00:00,  1.73ba/s]Running tokenizer on dataset: 100%|██████████| 68/68 [00:40<00:00,  2.00ba/s]Running tokenizer on dataset: 100%|██████████| 68/68 [00:40<00:00,  1.67ba/s]
Running tokenizer on dataset:   0%|          | 0/1 [00:00<?, ?ba/s]08/31/2022 18:40:59 - INFO - datasets.arrow_dataset - Caching processed dataset at /home2/longxin/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-ac4ad058892794e3.arrow
Running tokenizer on dataset: 100%|██████████| 1/1 [00:00<00:00,  1.36ba/s]Running tokenizer on dataset: 100%|██████████| 1/1 [00:00<00:00,  1.36ba/s]
Running tokenizer on dataset:   0%|          | 0/2 [00:00<?, ?ba/s]08/31/2022 18:41:00 - INFO - datasets.arrow_dataset - Caching processed dataset at /home2/longxin/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-d22e4641747e995b.arrow
Running tokenizer on dataset:  50%|█████     | 1/2 [00:00<00:00,  1.26ba/s]Running tokenizer on dataset: 100%|██████████| 2/2 [00:01<00:00,  1.60ba/s]Running tokenizer on dataset: 100%|██████████| 2/2 [00:01<00:00,  1.54ba/s]
08/31/2022 18:41:03 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/2.4.0/metrics/glue/glue.py not found in cache or force_download set to True, downloading to /home2/longxin/.cache/huggingface/datasets/downloads/tmp8uzb_jop
Downloading builder script:   0%|          | 0.00/1.84k [00:00<?, ?B/s]Downloading builder script: 5.76kB [00:00, 3.08MB/s]                   
08/31/2022 18:41:04 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/2.4.0/metrics/glue/glue.py in cache at /home2/longxin/.cache/huggingface/datasets/downloads/abffb8edcb06043a73f730df94f3e14efbe970a668a98bad1cba767fa03da126.c2504e8be3fadd1d3b046519eb7f546ac2d3655ba91d4dbf7b75247b76e1abb1.py
08/31/2022 18:41:04 - INFO - datasets.utils.file_utils - creating metadata file for /home2/longxin/.cache/huggingface/datasets/downloads/abffb8edcb06043a73f730df94f3e14efbe970a668a98bad1cba767fa03da126.c2504e8be3fadd1d3b046519eb7f546ac2d3655ba91d4dbf7b75247b76e1abb1.py
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Traceback (most recent call last):
  File "/home2/longxin/Neural_Coder_EXT/tmp.py", line 740, in <module>
    main()
  File "/home2/longxin/Neural_Coder_EXT/tmp.py", line 507, in main
    from neural_compressor.conf.config import QuantConf
  File "/home2/longxin/ls/envs/jupyterlab-ext/lib/python3.9/site-packages/neural_compressor-1.11-py3.9-linux-x86_64.egg/neural_compressor/__init__.py", line 22, in <module>
    from .contrib import *
  File "/home2/longxin/ls/envs/jupyterlab-ext/lib/python3.9/site-packages/neural_compressor-1.11-py3.9-linux-x86_64.egg/neural_compressor/contrib/__init__.py", line 18, in <module>
    from .strategy import *
  File "/home2/longxin/ls/envs/jupyterlab-ext/lib/python3.9/site-packages/neural_compressor-1.11-py3.9-linux-x86_64.egg/neural_compressor/contrib/strategy/__init__.py", line 25, in <module>
    __import__(basename(f)[:-3], globals(), locals(), level=1)
  File "/home2/longxin/ls/envs/jupyterlab-ext/lib/python3.9/site-packages/neural_compressor-1.11-py3.9-linux-x86_64.egg/neural_compressor/contrib/strategy/sigopt.py", line 21, in <module>
    from sigopt import Connection
  File "/home2/longxin/ls/envs/jupyterlab-ext/lib/python3.9/site-packages/sigopt/__init__.py", line 7, in <module>
    from .magics import SigOptMagics as _Magics
  File "/home2/longxin/ls/envs/jupyterlab-ext/lib/python3.9/site-packages/sigopt/magics.py", line 15, in <module>
    from .cli.commands.config import API_TOKEN_PROMPT, LOG_COLLECTION_PROMPT, CELL_TRACKING_PROMPT
  File "/home2/longxin/ls/envs/jupyterlab-ext/lib/python3.9/site-packages/sigopt/cli/__init__.py", line 1, in <module>
    from sigopt.cli.commands import sigopt_cli as cli
  File "/home2/longxin/ls/envs/jupyterlab-ext/lib/python3.9/site-packages/sigopt/cli/commands/__init__.py", line 1, in <module>
    import sigopt.cli.commands.cluster
  File "/home2/longxin/ls/envs/jupyterlab-ext/lib/python3.9/site-packages/sigopt/cli/commands/cluster/__init__.py", line 1, in <module>
    import sigopt.cli.commands.cluster.base
  File "/home2/longxin/ls/envs/jupyterlab-ext/lib/python3.9/site-packages/sigopt/cli/commands/cluster/base.py", line 3, in <module>
    from sigopt.orchestrate.controller import OrchestrateController
  File "/home2/longxin/ls/envs/jupyterlab-ext/lib/python3.9/site-packages/sigopt/orchestrate/controller.py", line 8, in <module>
    from botocore.exceptions import NoRegionError
ModuleNotFoundError: No module named 'botocore.exceptions'
